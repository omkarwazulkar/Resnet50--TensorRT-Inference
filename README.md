# Resnet50--TensorRT-Inference
This project performs high-performance inference with ResNet-50 using TensorRT. It starts by obtaining a pre-trained ResNet-50 model, uses NVIDIAâ€™s TensorRT Docker image from NGC, converts the model from PyTorch to ONNX, then builds a TensorRT engine, enabling extremely fast &amp; efficient inference with low latency &amp; high throughput on NVIDIA GPUs.
